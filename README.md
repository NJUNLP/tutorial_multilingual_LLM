# Research and Challenges of Multilingual Large Language Models 
NLPCC'2024, Tutorial Slides [[download]](https://owennju.github.io/archive/NLPCC2024_slides.pdf) 

Speaker: [Shujian Huang](http://nlp.nju.edu.cn/huangsj/), [Wenhao Zhu](https://owennju.github.io/) 

Affiliation: School of Computer Science, Nanjing University

## Paper List

### Latest Multilingual Models üéÉ
- [√úst√ºn et al. Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model, arXiv'2024](https://arxiv.org/pdf/2402.07827)
- [Alves et al., Tower: An Open Multilingual Large Language Model for Translation-Related Tasks, arXiv‚Äô2024](https://arxiv.org/pdf/2402.17733)
- [Mistral Team, Mistral Large, 2024](https://mistral.ai/news/mistral-large/)
- [Gemini Team, Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, arXiv‚Äô2024](https://arxiv.org/pdf/2403.05530)
- [OpenAI, Hello GPT-4o, 2024](https://openai.com/index/hello-gpt-4o/)
- [Lu et al., LLaMAX: Scaling Linguistic Horizons of LLM by Enhancing Translation Capabilities Beyond 100 Languages, Findings of EMNLP‚Äô2024](https://arxiv.org/pdf/2407.05975)
- [Nguyen et al., SeaLLMs - Large Language Models for Southeast Asia, ACL'2024](https://aclanthology.org/2024.acl-demos.28.pdf)
- [Aryabumi et al. Aya 23: Open Weight Releases to Further Multilingual Progress, arXiv'2024](https://cohere.com/research/aya/aya-23-technical-report.pdf)
- [LLaMA Team, The Llama 3 Herd of Models, arXiv‚Äô2024](https://arxiv.org/pdf/2407.21783)
- [Sun et al., FuxiTranyu: A Multilingual Large Language Model Trained with Balanced Data, arXiv'2024](https://arxiv.org/pdf/2408.06273)
- [Qwen Team, Qwen2.5: A Party of Foundation Models, 2024](https://qwenlm.github.io/blog/qwen2.5/)
- [Ji et al. EMMA-500: Enhancing Massively Multilingual Adaptation of Large Language Models. arXiv'2024](https://arxiv.org/pdf/2409.17892)
- [Xu et al. X-ALMA: Plug & Play Modules and Adaptive Rejection for Quality Translation at Scale. arXiv'2024](https://arxiv.org/pdf/2410.03115)
- [Yue et al. Pangea: A Fully Open Multilingual Multimodal Llm for 39 Languages. arXiv‚Äô2024.](https://arxiv.org/pdf/2410.16153)

### Multilingual Pre-training ‚õµÔ∏è
#### Data
- [Banon et al., ParaCrawl: Web-Scale Acquisition of Parallel Corpora, ACL‚Äô2020](https://aclanthology.org/2020.acl-main.417.pdf)
- [Conneau et al., Unsupervised Cross-lingual Representation Learning at Scale, ACL'2020](https://aclanthology.org/2020.acl-main.747.pdf)
- [Schwenk et al., WikiMatrix: Mining 135M Parallel Sentences in 1620 Language Pairs from Wikipedia, EACL'2021](https://aclanthology.org/2021.eacl-main.115.pdf)
- [Burchell et al., An Open Dataset and Model for Language Identification, ACL'2023](https://aclanthology.org/2023.acl-short.75.pdf)
- [Yuan et al., Lego-MT: Learning Detachable Models for Massively Multilingual Machine Translation, Findings of ACL‚Äô2023](https://aclanthology.org/2023.findings-acl.731.pdf)
- [Kudugunta et al, MADLAD-400: A Multilingual And Document-Level Large Audited Dataset. NeurIPS‚Äô2023](https://proceedings.neurips.cc/paper_files/paper/2023/file/d49042a5d49818711c401d34172f9900-Paper-Datasets_and_Benchmarks.pdf)
- [Arefyev et al., HPLT‚Äôs First Release of Data and Models, EAMT'2024](https://aclanthology.org/2024.eamt-2.27.pdf)
- [Ji et al., EMMA-500: Enhancing Massively Multilingual Adaptation of Large Language Models, arXiv‚Äô2024](https://arxiv.org/pdf/2409.17892)

#### Method
- [Li et al., PREALIGN: Boosting Cross-Lingual Transfer by Early Establishment of Multilingual Alignment, EMNLP'2024](https://arxiv.org/pdf/2407.16222)
- [He et al., Scaling Laws for Multilingual Language Models. arXiv‚Äô2024](https://arxiv.org/pdf/2410.12883)

### Multilingual Post-training üéØ
#### Data 
- [Muennighoff et al., Crosslingual Generalization through Multitask Finetuning, ACL‚Äô2023](https://aclanthology.org/2023.acl-long.891.pdf)
- [Li et al., Bactrian-X : A Multilingual Replicable Instruction-Following Model with Low-Rank Adaptation. arXiv‚Äô2023](https://arxiv.org/pdf/2305.15011)
- [Lai et al., Okapi: Instruction-tuned Large Language Models in Multiple Languages with Reinforcement Learning from Human Feedback. EMNLP‚Äô2023](https://aclanthology.org/2023.emnlp-demo.28.pdf)
- [Chen et al., Breaking Language Barriers in Multilingual Mathematical Reasoning: Insights and Observations. arXiv‚Äô2023](https://arxiv.org/pdf/2310.20246)
- [Lai et al., LLMs Beyond English: Scaling the Multilingual Capability of LLMs with Cross-Lingual Feedback, Findings of ACL'2024](https://aclanthology.org/2024.findings-acl.488.pdf)
- [Singh et al. Aya Dataset: An Open-Access Collection for Multilingual Instruction Tuning. arXiv, 2024](https://aclanthology.org/2024.acl-long.620.pdf)
#### Method
- [Shi et al., Language Models Are Multilingual Chain-Of-Thought Reasoners, ICLR‚Äô2023](https://openreview.net/pdf?id=fR3wGCk-IXp)
- [Huang et al., Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting, Findings of EMNLP, 2023](https://aclanthology.org/2023.findings-emnlp.826.pdf)
- [Qin et al., Cross-lingual Prompting: Improving Zero-shot Chain-of-Thought Reasoning across Languages. EMNLP, 2023](https://aclanthology.org/2023.emnlp-main.163.pdf)
- [Zhu et al., Question Translation Training for Better Multilingual Reasoning, Findings of ACL‚Äô2024](https://aclanthology.org/2024.findings-acl.498.pdf)
- [She et al., MAPO: Advancing Multilingual Reasoning through Multilingual Alignment-as-Preference Optimization, ACL‚Äô2024](https://aclanthology.org/2024.acl-long.539.pdf)
- [Zhang et al., PLUG: Leveraging Pivot Language in Cross-Lingual Instruction Tuning, ACL'2024](https://aclanthology.org/2024.acl-long.379.pdf)
- [Yoon et al., LangBridge: Multilingual Reasoning Without Multilingual Supervision, ACL‚Äô2024](https://aclanthology.org/2024.acl-long.405.pdf)
- [Zhao et al., LLaMA Beyond English: An Empirical Study on Language Capability Transfer, arXiv‚Äô2024](https://arxiv.org/pdf/2401.01055)
- [Geng et al., Why Not Transform Chat Large Language Models to Non-English?, arXiv'2024](https://arxiv.org/pdf/2405.13923)
- [Zhu et al. The Power of Question Translation Training in Multilingual Reasoning: Broadened Scope and Deepened Insights, arXiv‚Äô2024](https://arxiv.org/pdf/2405.01345)
- [Huang et al., MindMerger: Efficient Boosting LLM Reasoning in non-English Languages, NeurIPS‚Äô2024](https://arxiv.org/pdf/2405.17386)
- [Zhou et al., MoE-LPR: Multilingual Extension of Large Language Models through Mixture-of-Experts with Language Priors Routing, arXiv‚Äô2024](https://arxiv.org/pdf/2408.11396)
- [Xu et al., X-ALMA: Plug & Play Modules and Adaptive Rejection for Quality Translation at Scale, arXiv‚Äô2024](https://arxiv.org/pdf/2410.03115)

### Evaluation & Analysis üìè
- [Shi et al., Language Models Are Multilingual Chain-Of-Thought Reasoners, ICLR‚Äô2023](https://openreview.net/pdf?id=fR3wGCk-IXp)
- [Qi et al., Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models, EMNLP‚Äô2023](https://aclanthology.org/2023.emnlp-main.658.pdf)
- [Zhu et al., Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis, Findings of NAACL‚Äô2024](https://aclanthology.org/2024.findings-naacl.176.pdf)
- [Bhattacharya & Bojar, Unveiling Multilinguality in Transformer Models: Exploring Language Specificity in Feed-Forward Networks, arXiv‚Äô2023](https://aclanthology.org/2023.blackboxnlp-1.9.pdf)
- [Kew et al., Turning English-centric LLMs Into Polyglots: How Much Multilinguality Is Needed? arXiv‚Äô2024](https://arxiv.org/pdf/2312.12683)
- [Chen et al., Monolingual or Multilingual Instruction Tuning: Which Makes a Better Alpaca, Findings of EACL'2024](https://aclanthology.org/2024.findings-eacl.90.pdf)
- [Kojima et al., On the Multilingual Ability of Decoder-based Pre-trained Language Models: Finding and Controlling Language-Specific Neurons, NAACL'2024](https://aclanthology.org/2024.naacl-long.384.pdf)
- [Gao et al., Multilingual Pretraining and Instruction Tuning Improve Cross-Lingual Knowledge Alignment, But Only Shallowly, NAACL‚Äô2024](https://aclanthology.org/2024.naacl-long.339.pdf)
- [Wendler et al., Do Llamas Work in English? On the Latent Language of Multilingual Transformers, ACL'2024](https://aclanthology.org/2024.acl-long.820.pdf)
- [Tang et al., Language-Specific Neurons: The Key to Multilingual Capabilities in Large Language Models, ACL‚Äô2024](https://aclanthology.org/2024.acl-long.309.pdf)
- [Shaham et al., Multilingual Instruction Tuning With Just a Pinch of Multilinguality, ACL‚Äô2024](https://aclanthology.org/2024.findings-acl.136.pdf)
- [Zhao et al., How do Large Language Models Handle Multilingualism? arXiv, arXiv'2024](https://arxiv.org/pdf/2402.18815)
- [Wu et al., Reuse Your Rewards: Reward Model Transfer for Zero-Shot Cross-Lingual Alignment, arXiv‚Äô2024](https://arxiv.org/pdf/2404.12318)
- [Wang et al., Sharing Matters: Analysing Neurons Across Languages and Tasks in LLMs, arXiv'2024](https://arxiv.org/pdf/2406.09265)
- [Chen et al., Is It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models? arXiv'2024](https://arxiv.org/pdf/2406.12822)
- [Marchisio et al., Understanding and Mitigating Language Confusion in LLMs, arXiv'2024](https://arxiv.org/pdf/2406.20052)
- [Zhu et al., Multilingual Contrastive Decoding via Language-agnostic Layers Skipping, Findings of EMNLP'2024](https://arxiv.org/pdf/2407.10795)
- [Gureja et al., M-REWARDBENCH: Evaluating Reward Models in Multilingual Settings, arXiv‚Äô2024](https://arxiv.org/pdf/2410.15522)